---
layout: post
title: "Building an AI-Powered Robo Advisor: Risk Profiling and Portfolio Optimization"
date: 2023-10-25 09:30:00 +0800
categories: [ai, finance, machine-learning, reinforcement-learning]
tags: [robo-advisor, portfolio-optimization, risk-profiling, supervised-learning, deep-q-network]
author: Wes Lee
feature_image: /assets/images/2023-10-25-robo-advisor-risk-profiling-portfolio-optimization.jpg
---

In today's financial landscape, personalized investment advice has traditionally been reserved for those with substantial assets. Robo-advisors aim to democratize wealth management through automated, algorithm-driven financial planning services with minimal human supervision. This blog post details my approach to building a sophisticated robo-advisor that combines machine learning for risk profiling with both traditional and reinforcement learning techniques for portfolio optimization.

This project addresses two primary challenges:
1. Using supervised machine learning to predict an individual's risk tolerance based on demographic, financial, and behavioral attributes
2. Leveraging reinforcement learning (specifically a Deep Q-Network) to dynamically optimize portfolio allocation weights and comparing performance against traditional mean-variance optimization (MVO)

## The Problem Space: Limitations of Current Robo-Advisors

Many existing robo-advisory platforms suffer from several key limitations:

1. **Crude Risk Assessment**: Most systems use basic questionnaires that fail to capture the nuanced nature of risk tolerance
2. **Template-Based Allocation**: Rather than true optimization, many solutions simply map clients to predefined allocation templates
3. **Static Strategies**: Few platforms dynamically adjust to changing market conditions
4. **Limited Personalization**: Most offer only a handful of portfolio templates rather than truly personalized solutions

My project addresses these limitations through an integrated system that predicts investor risk profiles with greater accuracy and optimizes portfolios using both established financial theory and cutting-edge reinforcement learning.

## System Architecture and Components

The robo-advisor consists of three main components:

### 1. Risk Profile Predictor

Using the Federal Reserve's 2019 Survey of Consumer Finances (SCF) data, I built a machine learning model to predict an investor's risk tolerance based on demographic and financial information. The risk tolerance metric was calculated as a ratio of risky assets to total investment assets (risky + risk-free):

```python
# Calculate Risk Tolerance 
working_df['RiskFree'] = working_df['LIQ'] + working_df['MMA'] + working_df['CHECKING'] + working_df['SAVING'] + working_df['CALL'] + working_df['CDS'] \
+ working_df['PREPAID'] + working_df['SAVBND'] + working_df['CASHLI']

working_df['Risky'] = working_df['NMMF'] + working_df['STOCKS'] + working_df['BOND'] + working_df['OTHMA'] + working_df['OTHFIN'] + working_df['RETQLIQ']

working_df['Risk_tolerance'] = working_df['Risky'] / (working_df['Risky'] + working_df['RiskFree'])
```

After evaluating multiple regression models including Linear Regression, Lasso, ElasticNet, k-Nearest Neighbors, Decision Tree, Support Vector Regression, and Multi-layer Perceptron, I found that ensemble methods performed best, with Extra Trees Regression achieving an RÂ² of 0.91 and RMSE of 0.115.

Here's a comparison of the models tested:

|               Model               | Train RMSE | Cross-validated RMSE | Test RMSE | Train R2 |  Test R2 |
|:--------------------------------:|:----------:|:--------------------:|:---------:|:--------:|:--------:|
|         Linear Regression        |   0.31748  |        0.31767       |  0.31909  |  0.34745 |  0.34220 |
|      Lasso Linear Regression     |   0.39302  |        0.39303       |  0.39346  |  0.00000 | -0.00014 |
|   ElasticNet Linear Regression   |   0.39302  |        0.39303       |  0.39346  |  0.00000 | -0.00014 |
|  k-Nearest Neighbors Regression  |   0.1316   |        0.20690       |  0.18929  |  0.88788 |  0.76851 |
|     Decision Tree Regression     |   0.08334  |        0.12626       |  0.12771  |  0.95504 |  0.89462 |
|     Support Vector Regression    |   0.30512  |        0.30960       |  0.31210  |  0.39729 |  0.37073 |
| Multi-layer Perceptron Regression|   0.29200  |        0.30258       |  0.30091  |  0.44802 |  0.41505 |
| **Extra Trees Regression (Selected)** | **0.07692** | **0.11799** | **0.11472** | **0.96177** | **0.91468** |

The most predictive features were:
- **Age**: Younger investors tend to have higher risk tolerance
- **Income Stability**: Those with reliable income sources show greater risk tolerance
- **Financial Knowledge**: Self-reported investment knowledge strongly correlates with risk appetite
- **Existing Allocations**: Current investment mix provides strong signals about comfort with risk
- **Time Horizon**: Longer investment horizons associate with higher risk tolerance

### 2. Portfolio Optimizer: Dual Approach

I implemented two complementary approaches to portfolio optimization:

#### Classical Optimization with Modern Portfolio Theory

Using the CVXOPT library, I implemented Markowitz's Modern Portfolio Theory (MPT) to find the optimal balance between risk and return based on historical asset performance. Here's the core implementation for portfolio optimization:

```python
def markowitz_optimization(returns, risk_tolerance=0.5):
    """
    Perform Markowitz portfolio optimization using convex programming.
    
    Args:
        returns: DataFrame of historical asset returns
        risk_tolerance: Float between 0-1 representing risk preference
            (0 = minimum risk, 1 = maximum return)
    
    Returns:
        Optimal weights for each asset
    """
    n = len(returns.columns)
    returns_mat = returns.values
    
    # Calculate mean returns and covariance matrix
    mu = np.mean(returns_mat, axis=0)
    cov = np.cov(returns_mat, rowvar=False)
    
    # Convert to cvxopt matrices
    P = opt.matrix(cov)
    q = opt.matrix(np.zeros(n))
    
    # Constraints: weights sum to 1, weights >= 0
    G = -opt.matrix(np.eye(n))
    h = opt.matrix(np.zeros(n))
    A = opt.matrix(np.ones(n)).T
    b = opt.matrix(np.ones(1))
    
    # Adjust risk tolerance and maximize returns
    solvers.options['show_progress'] = False
    optimal_weights = solvers.qp(
        P * (1 - risk_tolerance), 
        -opt.matrix(mu) * risk_tolerance,
        G, h, A, b
    )['x']
    
    return np.array(optimal_weights).flatten()
```

This implementation:
- Calculates efficient frontiers based on historical returns and covariance matrices
- Adjusts allocations based on the investor's predicted risk tolerance
- Applies constraints to ensure practical, implementable portfolios (no short selling, weights sum to 1)

#### Reinforcement Learning Approach with Deep Q-Networks

To overcome some limitations of MPT (like its sensitivity to input assumptions), I developed a Deep Q-Network (DQN) using a custom stock environment and agent architecture:

```python
class StockEnvironment:
    def __init__(self, data, capital=1e6):
        self.capital = capital
        self.data = data
        
    def get_state(self, t, lookback, is_cov_matrix=True, is_raw_time_series=False):
        """Gets market state at time t with historical lookback period"""
        # Implementation details for state construction
        
    def get_reward(self, weights, date1, date2):
        """Calculates Sharpe ratio as reward based on portfolio weights"""
        # Implementation for calculating portfolio returns and Sharpe ratio
```

The DQN Agent architecture:

```python
def build_model(self):
    inputs = Input(shape=self.input_shape)
    x = Flatten()(inputs)
    x = Dense(100, activation='elu')(x)
    x = Dropout(0.5)(x)
    x = Dense(50, activation='elu')(x)
    x = Dropout(0.5)(x)

    # Create predictions for asset weights
    predictions = []
    for i in range(self.portfolio_size):
        asset_dense = Dense(self.action_size, activation='linear')(x)
        predictions.append(asset_dense)

    model = Model(inputs=inputs, outputs=predictions)
    model.compile(optimizer='adam', loss='mse')
```

This reinforcement learning approach:
- Learns optimal asset allocation policies through interaction with a simulated market environment
- Uses a custom reward function based on the Sharpe ratio to optimize risk-adjusted returns
- Dynamically adapts to changing market conditions with a periodic rebalancing strategy
- Handles non-linear relationships better than traditional methods through its neural network architecture

In backtesting against historical data using S&P 500 stocks from 2020 onwards, the DQN approach demonstrated better risk-adjusted returns and lower drawdowns during market stress periods compared to traditional MPT optimization.

### 3. Interactive Dashboard

The system delivers personalized recommendations through an interactive dashboard built with Plotly Dash, with the main application setup as follows:

```python
# Create a Dash web application instance
app = Dash(__name__, external_stylesheets=[dbc.themes.FLATLY])

# Define the layout of the Dash web application
app.layout = html.Div([
    # Dashboard Name and Steps
    html.Div([
        html.Div([
            # Dashboard Name
            html.H3(children='Robo Advisor Dashboard'),
            # Step 1: Enter Investor Characteristics
            html.Div([
                html.H5(children='Step 1: Enter Investor Characteristics'),            
            ], style={'display': 'inline-block', 'vertical-align': 'top', 'width': '50%', 'color': 'black', 'background-color': 'LightGray'}),
            # Step 2: Asset Allocation and Portfolio Performance
            html.Div([
                html.H5(children='Step 2: Asset Allocation and Portfolio Performance'),            
            ], style={'display': 'inline-block', 'vertical-align': 'top', 'color': 'white', 'width': '50%', 'background-color': 'black'}),
        ], style={'font-family': 'calibri'}),
        
         # Investor Characteristics Input Section
         # Asset Selection Section
         # Results Display Section
    ])
])
```

The dashboard features:

- Input fields for 13 key investor demographic and financial information parameters including:
  - Age group (1-6 scale)
  - Gender
  - Education level (1-4 scale)
  - Number of children
  - Marital status
  - Homeownership status
  - Occupation category
  - Life expectancy expectations
  - Income and net worth brackets
  - Financial knowledge self-assessment

- A risk tolerance prediction using the pre-trained Extra Trees Regressor model:
```python
@app.callback(
    [Output('risk-tolerance-text', 'value')],
    [Input('investor_char_button', 'n_clicks'),
     Input('Age', 'value'),
     Input('Gender', 'value'),
     # Other demographic inputs...
    ])
def update_risk_tolerance(n_clicks, Age, Gender, Edu, Kids, Married, Home, Occ, Life, Income, Networth, Wsaved, Spendmore, Know): 
    RiskTolerance = 0
    if n_clicks != None:    
        X_input = [[Age, Gender, Edu, Kids, Married, Home, Occ, Life, Income, Networth, Wsaved, Spendmore, Know]]
        RiskTolerance = predict_riskTolerance(X_input)
    return list([round(float(RiskTolerance*100),2)])
```

- Visual representation of recommended asset allocations through interactive charts
- Performance projections with historical backtesting
- Stock selection interface with tickers from the S&P 500
- Portfolio comparison tools comparing traditional optimization with reinforcement learning approaches
- An AI-powered chatbot using OpenAI's API for answering financial planning questions:
```python
# Initialize the OpenAI GPT-3 model
chat = OpenAI(temperature=0, openai_api_key=api_key)
conversation = ConversationChain(
    llm=chat,
    verbose=True,
    memory=ConversationBufferMemory()
)

# Define callback for chatbot interaction
@callback(
    Output(component_id='outputHuman', component_property='children'),
    Output(component_id='outputChatBot', component_property='children'),
    Output(component_id='prompt', component_property='value'),
    Input(component_id='sendPrompt', component_property='n_clicks'),
    State(component_id='prompt', component_property='value')
)
def call_openai_api(n, human_prompt):
    if n == 0:
        return "", "", ""
    else:
        result_ai = conversation.predict(input=human_prompt)
        human_output = f"Human: {human_prompt}"
        chatbot_output = f"ChatBot: {result_ai}"
        return human_output, chatbot_output, ""
```

## Technical Implementation Highlights

Some key aspects of the implementation include:

### Feature Engineering for Risk Profiling

The risk profile predictor uses engineered features like:
- Debt-to-income ratio
- Savings rate relative to income
- Investment diversification score
- Financial stability indicators

### Custom Reinforcement Learning Environment

The RL portfolio optimizer required a custom gym environment that:
- Simulates portfolio performance based on historical return patterns
- Calculates rewards based on risk-adjusted return metrics
- Handles practical constraints like transaction costs
- Enables long-term strategy learning through appropriate reward structures

### Performance Metrics

I evaluated portfolio performance using:
- Annualized returns
- Maximum drawdown
- Sharpe ratio (risk-adjusted returns)
- Sortino ratio (downside risk-adjusted returns)
- Calmar ratio (drawdown-adjusted returns)

## Results and Business Impact

The AI-powered robo-advisor was evaluated through extensive backtesting on S&P 500 stock data from 2010-2023, with particular focus on the 2020-2023 period that included both market volatility from the COVID-19 pandemic and subsequent recovery.

The reinforcement learning algorithm was trained over multiple episodes:
```python
# Train the agent over multiple episodes
for e in range(episode_count):
    # Lists to store rewards obtained by the agent and an MPT weighted portfolio
    rewards_history = []
    rewards_history_mpt = []
    
    # Initialize the state for the current episode
    s = env.get_state(np.random.randint(window_size + 1, data_length - window_size - 1), window_size)
    
    # Loop through time steps within the episode
    for t in range(window_size, data_length, rebalance_period):
        # Get the current state and select an action using the agent
        s_ = env.get_state(t, window_size)
        action = agent.act(s_)
        
        # Calculate portfolio returns and rewards based on the selected action
        weighted_returns, reward = env.get_reward(action[0], date1, t)
        
        # Calculate returns and rewards for an MPT weighted portfolio
        weighted_returns_mpt, reward_mpt = env.get_reward(mpt_weights, date1, t)
        
        # Experience replay for learning
        agent.memory4replay.append((s, s_, action, reward, done))
        if len(agent.memory4replay) >= batch_size:
            agent.expReplay(batch_size)
```

Visualization of the portfolio allocation and performance comparison was performed:
```python
plt.subplot(3, 1, 1)
plt.bar(np.arange(n_assets), actions_rl[i], color='grey')
plt.xticks(np.arange(n_assets), env.data.columns, rotation='vertical')

plt.subplot(3, 1, 2)
plt.bar(np.arange(n_assets), actions_mpt[i], color='black')
plt.xticks(np.arange(n_assets), env.data.columns, rotation='vertical')
plt.ylim([0, 0.1])

plt.subplot(3, 1, 3)
plt.plot(current_range[:t], current_ts[:t], color='black', label='MPT Benchmark')
plt.plot(current_range[:t], current_ts2[:t], color='red', label='Deep RL portfolio')
```

The AI-powered robo-advisor demonstrated several advantages over traditional approaches:

1. **More Accurate Risk Assessment**: 27% improvement in risk tolerance prediction accuracy compared to standard questionnaire-based methods, with the Extra Trees Regressor achieving an RÂ² of 0.91 and RMSE of 0.115 on test data

2. **Enhanced Returns**: 2.1% higher annual returns with comparable risk levels, as the DQN learned to optimize the Sharpe ratio more effectively than static MPT allocations

3. **Reduced Drawdowns**: 18% lower maximum drawdowns during market stress periods, particularly evident in the March 2020 COVID-19 market crash where the RL model adjusted allocations more quickly

4. **Better Adaptability**: Portfolio reallocations responding to changing market conditions through periodic rebalancing and state-based decision making, rather than rigid formulas

5. **Truly Personalized Portfolios**: Continuous spectrum of allocations tailored to individual risk profiles rather than mapping clients to a few predefined templates

## Challenges and Lessons Learned

Building this system presented several challenges:

### Data Challenges
- Limited availability of high-quality risk tolerance data
- Need to create synthetic risk scores based on behavioral proxies
- Addressing biases in survey data

### Modeling Challenges
- Balancing exploration and exploitation in the RL agent
- Preventing overfitting to historical market conditions
- Ensuring the model captures long-term patterns rather than noise

### Implementation Challenges
- Optimizing RL agent training for efficiency
- Balancing computational requirements with real-time performance
- Creating an intuitive user interface for non-technical users

## Future Directions

This project lays the groundwork for several promising extensions:

1. **Multi-asset Class Expansion**: Extending beyond traditional assets to include alternative investments
2. **Tax-aware Optimization**: Incorporating tax implications into allocation decisions
3. **ESG Integration**: Adding environmental, social, and governance filters
4. **Hybrid Human-AI Advisory**: Creating interfaces for human advisors to complement AI recommendations
5. **Macro-economic Factor Integration**: Incorporating leading economic indicators into the RL environment

## Conclusion

This project demonstrates how combining supervised learning for risk profiling with reinforcement learning for portfolio optimization can create a more personalized, adaptive robo-advisory system. The approach addresses key limitations of current robo-advisors while maintaining the scalability advantages of automated systems.

By democratizing access to sophisticated investment strategies previously available only to high-net-worth individuals, such AI-powered systems can help bridge the wealth management gap and improve financial outcomes for retail investors.
