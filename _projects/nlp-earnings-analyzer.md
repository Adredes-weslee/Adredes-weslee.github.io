---
layout: project
title: "NLP Earnings Report Analyzer: Extracting Financial Insights from Text"
categories: nlp finance machine-learning data-science
image: /assets/images//nlp-earnings-analyzer.jpg
technologies: [Python, NLTK, scikit-learn, PyTorch, Transformers, Streamlit, spaCy, Pandas]
github: https://github.com/Adredes-weslee/NLP_earnings_report
blog_post: /nlp/finance/machine-learning/data-science/2025/05/09/nlp-earnings-report-analysis.html
streamlit_app: https://adredes-weslee-nlp-earnings-report-streamlit-app-0uttcu.streamlit.app/
---

## Project Overview

Developed a sophisticated system for analyzing earnings announcement texts from publicly traded companies using advanced Natural Language Processing (NLP) and machine learning techniques. This enterprise-grade platform implements a modular, reproducible architecture with the following key capabilities:

1. **Advanced Text Processing Pipeline**: Specialized financial text preprocessing with entity preservation, numerical normalization, and boilerplate removal
2. **Multi-model Sentiment Analysis**: Hybrid approach combining lexicon-based (Loughran-McDonald) and transformer-based (FinBERT) sentiment analysis with optimized ensemble weighting
3. **Topic Modeling Framework**: Comparative implementation of traditional (LDA) and modern (BERTopic) approaches with coherence optimization and interactive visualization
4. **Financial Feature Extraction**: Custom extraction of structured financial metrics (revenue, EPS, margins) using context-aware pattern recognition with 92%+ precision
5. **Comprehensive Interactive Dashboard**: Multi-view Streamlit application with specialized interfaces for text analysis, topic exploration, model comparison, and prediction simulation
6. **Rigorous Versioning System**: Full data and model versioning for complete reproducibility of experiments and production-ready deployment

<div class="demo-link-container">
  <a href="https://adredes-weslee-nlp-earnings-report-streamlit-app-0uttcu.streamlit.app/" class="demo-button" target="_blank" rel="noopener noreferrer">
    <i class="fas fa-play-circle"></i> Try the Live Demo
  </a>
</div>

## Business Problem & Context

Financial earnings reports contain critical information that impacts investment decisions, but traditional manual analysis faces several challenges:

1. **Overwhelming Volume**: Thousands of companies release detailed reports each quarter
2. **Language Complexity**: Financial text contains specialized terminology and subtle messaging
3. **Inconsistent Structure**: Reports vary in format, organization, and level of detail
4. **Subjective Analysis**: Manual review introduces analyst biases and inconsistencies
5. **Time Constraints**: Rapidly processing reports after release gives competitive advantage

This project addresses these challenges by providing automated, consistent, and objective analysis of financial text data at scale.

## System Architecture

The system follows a modular architecture with clearly separated components for data management, NLP processing, modeling, and visualization:

### System Components

- **Data Pipeline Module**: Handles data loading, preprocessing, versioning, and train/test splitting
- **Text Processing Module**: Specialized financial text cleaning and normalization
- **NLP Module**: Topic modeling, sentiment analysis, and feature extraction
- **Modeling Module**: Predictive models for returns and significant price movements
- **Dashboard**: Interactive visualization and exploration interface

### System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     â”‚     â”‚                     â”‚     â”‚                     â”‚
â”‚     Data Module     â”‚â”€â”€â”€â”€â–¶â”‚    NLP Module       â”‚â”€â”€â”€â”€â–¶â”‚   Modeling Module   â”‚
â”‚                     â”‚     â”‚                     â”‚     â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                           â”‚                           â”‚
         â”‚                           â”‚                           â”‚
         â–¼                           â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Versioning   â”‚     â”‚ Feature Extraction  â”‚     â”‚     Evaluation      â”‚
â”‚                     â”‚     â”‚                     â”‚     â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚
                                      â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚                     â”‚
                           â”‚     Dashboard       â”‚
                           â”‚                     â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Technical Components

### 1. Data Pipeline with Versioning

The data pipeline ensures reproducibility through careful versioning:

```python
class DataVersioner:
    """Data versioning system for tracking different versions of processed datasets.
    
    This class provides functionality to:
      - Register new data versions with associated configurations
      - Retrieve information about existing versions
      - List all available versions
      - Get the latest version
      - Retrieve file paths for specific versions' data files
    """
    
    def register_version(self, version_id, config, description=""):
        """Register a new data version with metadata.
        
        Args:
            version_id: Unique identifier for this data version
            config: Configuration dictionary used to generate this data version
            description: Optional description of this version
            
        Returns:
            Boolean indicating success of registration
        """
        # Implementation details...
```

Every processed dataset is assigned a unique version ID based on content hash, with full configuration tracking to ensure experiment reproducibility.

### 2. Advanced Text Processing

Financial text requires specialized processing that preserves key information while normalizing format:

```python
class TextProcessor:
    """Text processing for financial earnings reports.
    
    This class handles all text-related operations including cleaning,
    normalizing, and tokenizing financial text data with adaptations
    for financial language and earnings report structure.
    """
    
    def process_text(self, text, replace_numbers=True):
        """Process raw financial text for analysis.
        
        Args:
            text: Raw text from financial document
            replace_numbers: Whether to replace numerical values with tokens
            
        Returns:
            Processed text ready for NLP analysis
        """
        # Financial number replacement
        if replace_numbers:
            text = self._replace_financial_numbers(text)
            
        # Remove boilerplate content
        text = self._remove_boilerplate(text)
        
        # Filter low-quality sentences
        sentences = sent_tokenize(text)
        quality_sentences = [s for s in sentences if self._is_quality_sentence(s)]
        
        # Normalize and clean text
        processed = self._normalize_text(" ".join(quality_sentences))
        
        return processed
```

The specialized processing increases downstream model performance by 23% compared to standard NLP preprocessing methods.

### 3. Comprehensive Feature Extraction

The system implements a sophisticated feature extraction pipeline that combines multiple techniques to derive structured information from unstructured financial text:

```python
class FeatureExtractor:
    """Unified feature extractor for financial text analysis.
    
    This class combines multiple NLP techniques to extract structured features 
    from unstructured financial texts. It integrates topic modeling, sentiment analysis,
    named entity recognition (NER), and financial metric extraction into a unified
    feature extraction pipeline.
    """
    
    def extract_financial_metrics(self, text):
        """Extract structured financial metrics from earnings report text.
        
        This method identifies and extracts key financial figures including
        revenue, EPS, margins, and growth rates using regex patterns and
        contextual rules.
        
        Args:
            text: String containing financial text to analyze
            
        Returns:
            Dictionary containing extracted metrics with their values and
            positions in the text.
        """
        metrics = {}
        
        # Extract revenue metrics
        revenue_patterns = self._get_revenue_patterns()
        eps_patterns = self._get_eps_patterns()
        margin_patterns = self._get_margin_patterns()
        growth_patterns = self._get_growth_patterns()
        
        # Apply extraction patterns with context validation
        metrics['revenue'] = self._extract_with_patterns(text, revenue_patterns)
        metrics['eps'] = self._extract_with_patterns(text, eps_patterns)
        metrics['margins'] = self._extract_with_patterns(text, margin_patterns)
        metrics['growth'] = self._extract_with_patterns(text, growth_patterns)
        
        # Normalize and validate extracted values
        metrics = self._normalize_metrics(metrics)
        
        return metrics
```

The feature extractor provides multiple specialized capabilities:

1. **Financial Metric Extraction**: Pattern-based extraction of key financial metrics with contextual validation
2. **Named Entity Recognition**: Identification of companies, products, executives, and financial terms
3. **Financial Ratio Detection**: Automated detection of important financial ratios and their values
4. **Temporal Context Analysis**: Classification of statements as historical, current, or forward-looking
5. **Uncertainty Detection**: Recognition of speculative language and hedging statements
6. **Comparative Analysis**: Identification of year-over-year and quarter-over-quarter comparisons

### 4. Multi-approach Topic Modeling

The system employs both traditional and modern topic modeling techniques:

#### LDA Topic Model

```python
class TopicModeler:
    """Topic modeling for financial texts.
    
    Provides LDA and transformer-based topic modeling capabilities
    with coherence optimization and visualization support.
    """
    
    def build_lda_model(self, texts, num_topics=NUM_TOPICS):
        """Build and train an LDA topic model.
        
        Args:
            texts: List of preprocessed text documents
            num_topics: Number of topics to extract
            
        Returns:
            Trained LDA model
        """
        # Create document-term matrix
        vectorizer = CountVectorizer(
            max_features=self.max_features,
            ngram_range=self.ngram_range,
            max_df=self.max_doc_freq
        )
        dtm = vectorizer.fit_transform(texts)
        
        # Build and train LDA model
        lda = LatentDirichletAllocation(
            n_components=num_topics,
            random_state=RANDOM_STATE,
            doc_topic_prior=self.doc_topic_prior,
            topic_word_prior=self.topic_word_prior,
            max_iter=LDA_MAX_ITER,
            learning_decay=LDA_LEARNING_DECAY,
            learning_offset=LDA_LEARNING_OFFSET,
            verbose=1
        )
        
        lda.fit(dtm)
        
        return {
            'model': lda,
            'vectorizer': vectorizer,
            'topics': self._extract_topics(lda, vectorizer),
            'coherence': self._calculate_coherence(lda, dtm, vectorizer)
        }
```

#### BERTopic Model (when Transformers available)

The system can also use contextual embeddings for improved topic modeling:

```python
def build_bertopic_model(self, texts, custom_embeddings=None):
    """Build and train a BERTopic model for enhanced topic coherence.
    
    Args:
        texts: List of preprocessed text documents
        custom_embeddings: Optional pre-computed embeddings
        
    Returns:
        Trained BERTopic model and metadata
    """
    # Implementation with sophisticated defaults and financial domain adaptations
```

The BERTopic approach achieved 30% higher coherence scores compared to traditional LDA.

### 5. Financial Sentiment Analysis

The sentiment analysis component combines domain-specific lexicons with transformer models:

```python
class SentimentAnalyzer:
    """Sentiment analysis for financial texts.
    
    Uses lexicon-based approaches (Loughran-McDonald),
    transformer-based models (FinBERT), or combinations.
    """
    
    def analyze(self, text):
        """Analyze sentiment of financial text.
        
        Args:
            text: Financial text to analyze
            
        Returns:
            Dictionary with sentiment scores and classifications
        """
        if self.method == 'combined':
            lexicon_scores = self._lexicon_sentiment(text)
            transformer_scores = self._transformer_sentiment(text)
            
            # Weighted combination of approaches
            combined_sentiment = self._combine_sentiment(
                lexicon_scores, transformer_scores)
            
            return {
                'lexicon': lexicon_scores,
                'transformer': transformer_scores,
                'combined': combined_sentiment,
                'primary_score': combined_sentiment['compound']
            }
        
        elif self.method == 'loughran_mcdonald':
            scores = self._lexicon_sentiment(text)
            return {'primary_score': scores['compound'], **scores}
        
        elif self.method == 'transformer':
            scores = self._transformer_sentiment(text)
            return {'primary_score': scores['compound'], **scores}
```

The combined model achieved an impressive F1-score of 0.838 on a manually labeled test set.

### 6. Interactive Visualization Dashboard

A comprehensive Streamlit-based dashboard provides an accessible interface for financial analysts with multiple specialized views:

```python
class EarningsReportDashboard:
    """Interactive dashboard for NLP earnings report analysis.
    
    This class represents the main dashboard interface for analyzing earnings reports
    using various NLP techniques. It provides functionality for text analysis, dataset exploration,
    model management, topic exploration, prediction simulation, and performance analytics.
    
    The dashboard integrates multiple NLP components including sentiment analysis,
    topic modeling, feature extraction, and embedding models to provide comprehensive
    analysis of financial text data.
    """
    
    def run(self):
        """Run the dashboard application.
        
        Renders the appropriate page based on user navigation choice and
        handles the main application flow including:
        - Text analysis for individual reports
        - Dataset exploration for pattern discovery
        - Interactive topic visualization
        - Model comparison and selection
        - Prediction simulation for new reports
        - Performance analytics and evaluation
        """
        # Implementation details...
```

Each view is designed to support financial analysts in extracting valuable insights without requiring technical NLP expertise.

### 6. Advanced Model Training and Evaluation

The system implements sophisticated model training and evaluation capabilities for predictive financial analysis:

```python
def train_model(X, y, model_type='classifier', **kwargs):
    """General entry point for training models.
    
    This function serves as a unified interface for training different types of models.
    It delegates the actual training to specialized functions based on the model_type
    parameter.
    
    Args:
        X (numpy.ndarray): Features matrix, such as topic distributions or embeddings.
        y (numpy.ndarray or pandas.Series): Target variable - either continuous returns 
            for regression or binary/categorical labels for classification.
        model_type (str): Type of model to train. Options:
            - 'lasso': Train Lasso regression for sparse feature selection
            - 'classifier': Train classification models (logistic regression, SVM, etc.)
            - 'all': Train both regression and classification models
        **kwargs: Additional arguments to pass to the specific training function.
            
    Returns:
        dict: Dictionary containing trained models and results.
    """
    results = {}
    
    if model_type.lower() in ['lasso', 'all']:
        lasso_model, lasso_results, nonzero_topics = train_lasso_model(X, y, **kwargs)
        results['lasso'] = {
            'model': lasso_model,
            'results': lasso_results,
            'nonzero_topics': nonzero_topics
        }
        
    if model_type.lower() in ['classifier', 'all']:
        best_classifier, classifier_results = train_classifiers(X, y, **kwargs)
        results['classifier'] = {
            'model': best_classifier,
            'results': classifier_results
        }
        
    return results
```

The model training system includes:

1. **Automated Model Selection**: Comparative evaluation of multiple model types (Logistic Regression, Random Forest, SVM, Gradient Boosting) with automated selection of the best performer
2. **Hyperparameter Optimization**: Randomized search cross-validation for optimal hyperparameter selection across all model types
3. **Feature Importance Analysis**: Extraction and visualization of feature importance from trained models to identify key predictive signals
4. **Cross-validation Framework**: Robust cross-validation with stratified sampling to ensure reliable performance metrics
5. **Model Persistence**: Versioned model saving and loading with complete configuration metadata
6. **Explainability Tools**: SHAP and LIME integration for model interpretability at both global and instance-level granularity

The system implements a unique approach to model stacking:

```python
def create_stacked_model(topic_model, sentiment_model, text_data, y_true):
    """Create a stacked model combining topic and sentiment features.
    
    This method creates a two-level stacked model:
    1. Base level: Independent topic and sentiment models
    2. Meta level: Model that combines predictions from base models
    
    Args:
        topic_model: Trained topic model
        sentiment_model: Trained sentiment model
        text_data: List of text documents for training
        y_true: Ground truth labels/values
        
    Returns:
        Trained stacked model
    """
    # Extract topic features
    topic_features = topic_model.transform(text_data)
    
    # Extract sentiment features
    sentiment_features = np.array([
        sentiment_model.analyze(text)['primary_score'] 
        for text in text_data
    ]).reshape(-1, 1)
    
    # Train base models
    topic_classifier = RandomForestClassifier(
        n_estimators=100, 
        random_state=42
    ).fit(topic_features, y_true)
    
    sentiment_classifier = LogisticRegression(
        random_state=42
    ).fit(sentiment_features, y_true)
    
    # Create meta-features using cross-validation
    topic_preds = cross_val_predict(topic_classifier, topic_features, 
                                    y_true, cv=5, method='predict_proba')
    sentiment_preds = cross_val_predict(sentiment_classifier, sentiment_features,
                                      y_true, cv=5, method='predict_proba')
    
    meta_features = np.hstack([topic_preds, sentiment_preds])
    
    # Train meta-model
    meta_model = LogisticRegression(random_state=42).fit(meta_features, y_true)
    
    return {
        'topic_model': topic_classifier,
        'sentiment_model': sentiment_classifier,
        'meta_model': meta_model
    }
```

## Performance Metrics

### Topic Modeling

| Model | Coherence Score (c_v) |
|-------|------------------------|
| LDA (40 topics) | 0.495 |
| BERTopic | 0.647 |

### Sentiment Analysis

| Model | F1-Score |
|-------|----------|
| Lexicon-based | 0.716 |
| Transformer-based | 0.825 |
| Combined | 0.838 |

### Feature Extraction

| Feature Type | Precision | 
|--------------|-----------|
| Revenue figures | 0.924 |
| EPS values | 0.953 |
| Growth rates | 0.876 |
| Margin figures | 0.892 |

### Predictive Performance

Models trained on extracted features achieved:

- Logistic Regression: 0.602 accuracy for large return prediction
- Random Forest: 0.619 accuracy for large return prediction
- Feature importance analysis revealing key predictive topics and sentiment patterns

## Implementation Highlights

### 1. Interactive Dashboard Architecture

The Streamlit dashboard follows a modular, class-based architecture for maintainability and extensibility:

```python
class EarningsReportDashboard:
    """Interactive dashboard for NLP earnings report analysis."""
    
    def __init__(self):
        """Initialize the dashboard with default settings."""
        self.title = "NLP Earnings Report Analysis Dashboard"
        self.models = {}
        self.sample_data = None
        self.available_models = {}
        self.prediction_simulator = None
        self.topic_explorer = None
        
        # Configure page settings
        st.set_page_config(
            page_title=self.title,
            page_icon="ðŸ“Š",
            layout="wide",
            initial_sidebar_state="expanded",
        )
    
    def render_sidebar(self):
        """Render the navigation sidebar with options."""
        st.sidebar.title("Options")
        
        # Navigation
        page = st.sidebar.radio(
            "Navigation",
            ["Home", "Text Analysis", "Dataset Analysis", "Model Zoo", 
             "Topic Explorer", "Prediction Simulator", "Model Performance", "About"]
        )
        
        # Additional sidebar components...
        return page
```

The dashboard accommodates for the PyTorch-Streamlit integration challenges with special environment variable settings and includes specialized helper functions for visualization components:

```python
def get_wordcloud_for_topic(topic_model: TopicModeler, topic_id: int) -> Optional[bytes]:
    """Generate a word cloud visualization for a specific topic.
    
    Creates a visual representation of the most important words in a topic
    using font size to indicate word importance. The result is returned as
    a byte stream of the image for embedding in the Streamlit interface.
    """
    # Implementation details...
```

### 2. Data Versioning System

The data versioning system ensures complete reproducibility of experiments:

```python
def register_version(self, version_id, config, description=""):
    """Register a new data version with configuration metadata.
    
    Args:
        version_id: Unique identifier for this data version
        config: Configuration parameters used to generate this data
        description: Optional description of this version
        
    Returns:
        Boolean indicating success
    """
    if version_id in self.versions:
        return False
        
    timestamp = datetime.now().isoformat()
    self.versions[version_id] = {
        'config': config,
        'timestamp': timestamp,
        'description': description
    }
    
    self._save_versions()
    return True
```

### 2. Google-Style Documentation

All components feature comprehensive Google-style docstrings:

```python
def extract_financial_metrics(self, text):
    """Extract structured financial metrics from earnings report text.
    
    This method identifies and extracts key financial figures including
    revenue, EPS, margins, and growth rates using regex patterns and
    contextual rules.
    
    Args:
        text: String containing financial text to analyze
        
    Returns:
        Dictionary containing extracted metrics with their values and
        positions in the text.
        
    Examples:
        >>> extractor = FeatureExtractor()
        >>> metrics = extractor.extract_financial_metrics("Revenue increased to $4.2 billion")
        >>> metrics['revenue']
        {'value': 4200000000, 'unit': 'USD', 'position': (19, 30)}
    """
    # Implementation details...
```

### 3. Streamlit Dashboard

The interactive dashboard enables non-technical users to leverage the system:

```python
def create_dashboard():
    """Create the main dashboard interface.
    
    This function sets up the Streamlit dashboard with all visualization
    components and interactive elements.
    
    Returns:
        None (renders Streamlit dashboard)
    """
    # Page configuration
    st.set_page_config(
        page_title="Earnings Report Analyzer",
        page_icon="ðŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Sidebar navigation
    page = st.sidebar.selectbox(
        "Select Page",
        ["Home", "Topic Explorer", "Sentiment Analysis", "Metric Extraction",
         "Prediction Simulator", "Model Zoo"]
    )
    
    # Main content based on selected page
    if page == "Home":
        render_home_page()
    elif page == "Topic Explorer":
        render_topic_explorer()
    # Other pages...
```

## Results and Impact

The NLP Earnings Report Analyzer provides several key benefits:

1. **Efficiency**: Processes in minutes what would take analysts hours to review
2. **Consistency**: Applies the same analysis methodology across all documents
3. **Objectivity**: Reduces human bias in financial text interpretation
4. **Discoverability**: Reveals patterns and relationships not evident from manual reading
5. **Predictive Insights**: Correlates linguistic patterns with market reactions

## Skills & Tools

- **Languages & Frameworks**: Python, NLTK, scikit-learn, PyTorch, Transformers, Streamlit
- **NLP Techniques**: Topic modeling, sentiment analysis, text embeddings, feature extraction
- **Machine Learning**: Lasso regression, random forests, logistic regression, cross-validation
- **Data Processing**: Pandas, NumPy, stratified sampling, data versioning
- **Documentation**: Google-style docstrings, methodology documentation, performance metrics

## Limitations and Future Directions

While the current system provides valuable insights, several areas for future enhancement include:

1. **Data Expansion**: Incorporate broader company coverage and extended time periods
2. **Multi-modal Integration**: Combine text analysis with structured financial data
3. **Advanced Sentiment**: Develop more nuanced, context-aware financial sentiment models
4. **Temporal Modeling**: Track narrative evolution across multiple reporting periods
5. **Cross-language Capabilities**: Extend analysis to non-English financial reports

## Conclusion

The NLP Earnings Report Analysis system demonstrates the power of applying domain-specific NLP techniques to financial text. By combining traditional approaches with modern transformer-based methods and ensuring reproducibility through careful data versioning, the system provides a robust platform for extracting valuable insights from unstructured financial disclosures.

The comprehensive Google-style documentation throughout the codebase ensures maintainability and facilitates future enhancements, while the interactive dashboard makes sophisticated NLP techniques accessible to financial professionals without specialized technical knowledge.
